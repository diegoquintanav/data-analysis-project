---
title: "Exploratory Analysis of Efficiency in Buildings Dataset"
author: "Diego Quintana, Manuel Breve, Marcel Pons"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header  theme: united
    df_print: kable
  pdf_document:
    toc: true
    toc_depth: 1  # upto three depths of headings (specified by #, ## and ###)
    fig_width: 6
    fig_height: 4
    fig_caption: true
    # df_print: kable
    # highlight: tango
    # geometry: margin=3cm
    # latex_engine: xelatex
---

```{r}
# imports for the project
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE)
library(mice) # for imputing missing data
library(psych)
library(ggplot2) # for an improved toolset for plots
library(tableplot)
library(reshape2)
library(chemometrics) # for the mahalanobis distance
library(pracma)
library(PerformanceAnalytics) # to plot histograms
library(dplyr) # for an improved toolset


set.seed(42)
# add a new comment here
```

# About

From UCI's Machine Learning repository description in https://archive.ics.uci.edu/ml/datasets/Energy+efficiency 

> Abstract: This study looked into assessing the heating load and cooling load requirements of buildings (that is, energy efficiency) as a function of building parameters.
	

> We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer.

![Summary from the paper](images/summary-paper.png)

```{r}
energy <- readxl::read_xlsx("data/ENB2012_data.xlsx")
head(energy)
```

> The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses.



```{r}
class(energy)
```

Specifically:
X1 Relative Compactness
X2 Surface Area
X3 Wall Area
X4 Roof Area
X5 Overall Height
X6 Orientation
X7 Glazing Area
X8 Glazing Area Distribution
y1 Heating Load
y2 Cooling Load

```{r}
colnames(energy)
```
```{r}
colnames(energy) <- c("X1.RelativeCompactness",
"X2.SurfaceArea",
"X3.WallArea",
"X4.RoofArea",
"X5.OverallHeight",
"X6.Orientation",
"X7.GlazingArea",
"X8.GlazingAreaDistribution",
"y1.HeatingLoad",
"y2.CoolingLoad")

head(energy)
```


```{r}
describe(energy)
```

## Converting categorical variables

Continous variables:
X1,X2,X3,X4,X5,X7

Categorical variables: 
X6: 4 types of orientation: North, East, South, West
X8: 6 types of glazing area distribution: Uniform, North, East, South, West and No Glazing areas.


> We used three types of glazing areas, which are expressed as
percentages of the floor area: 10%, 25%, and 40%. Furthermore, five
different distribution scenarios for each glazing area were simu-
lated: (1) uniform: with 25% glazing on each side, (2) north: 55% on
the north side and 15% on each of the other sides, (3) east: 55% on
the east side and 15% on each of the other sides, (4) south: 55% on
the south side and 15% on each of the other sides, and (5) west: 55%
on the west side and 15% on each of the other sides. In addition,
we obtained samples with no glazing areas. Finally, all shapes were
rotated to face the four cardinal points.


```{r}
energy$X6.Orientation <- factor(energy$X6.Orientation, levels=2:5, labels = c('North','East', 'South','West'))
energy$X8.GlazingAreaDistribution <- factor(energy$X8.GlazingAreaDistribution, levels = 0:5, labels = c("No Glazing", "Uniform", 'North','East', 'South','West'))
```


## Separating in continuous and categorical data

```{r}
energy.continous <- select(energy, -grep("^X[6,8]", colnames(energy)))
```

## Separating datasets in inputs and response variables

```{r}
energy.X <- energy[,-grep("Load$", colnames(energy.continous))]
energy.y <- energy[,grep("Load$", colnames(energy.continous))]
```


## Imputing missing data

```{r}
md.pattern(energy, plot = FALSE)
```


## Data visualizations

```{r}
# https://datacritics.com/2018/02/28/melt-your-data-for-fast-visuals-with-your-dataset-in-r/
melt.energy <- melt(energy)
ggplot(data = melt.energy, aes(x = value)) + 
  stat_density() + 
  facet_wrap(~variable, scales = "free")
```


## Outlier Detection

### Univariate outlier analysis

```{r}
# https://stackoverflow.com/questions/44089894/identifying-the-outliers-in-a-data-set-in-r
OutVals <-  boxplot(energy)$out
which(energy %in% OutVals)
```

### Using mahalanobis distances (covariance matrix is singular, find out what it means)

```{r eval=FALSE}
# normalize dataset
m <- energy.X
m <- scale(m, center=TRUE, scale=colSums(m))
Moutlier(m, quantile = 0.975, plot = TRUE)
```

This matrix is not invertible. Two fundamental linear algebra properties:

- A singular (square) matrix is a (square) matrix that is not invertible.
- A matrix is not invertible if its determinant equals zero.

We can check the condition number

```{r eval=FALSE}
# https://stackoverflow.com/questions/50928796/system-is-computationally-singular-reciprocal-condition-number-in-r
# https://en.wikipedia.org/wiki/Condition_number
# https://stats.stackexchange.com/questions/37743/singular-covariance-matrix-in-mahalanobis-distance-in-matlab
kappa(as.matrix(energy.X))
```

```{r, include=TRUE, echo=TRUE }
mahalanobis2 <- function(energy,
                         inv.pseudo = FALSE,
                         robust = TRUE,
                         plot = TRUE,
                         tol = 0.975) {
  # we can use the penrose-moore pseudoinverse instead of the normal inverse
  # https://stats.stackexchange.com/a/37810/101414
  
  # cite this as
  # kjetil b halvorsen (https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen), Singular covariance matrix in Mahalanobis distance in Matlab, URL (version: 2012-09-22): https://stats.stackexchange.com/q/37810
  
  if (robust == TRUE) {
    # Uses the MCD estimator, according to
    
    # Mia Hubert, Peter J. Rousseeuw, Stefan Van Aelst,
    # 10 - Multivariate Outlier Detection and Robustness,
    # Editor(s): C.R. Rao, E.J. Wegman, J.L. Solka,
    # Handbook of Statistics,
    # Elsevier,
    # Volume 24,
    # 2005,
    # Pages 263-302,
    # ISSN 0169-7161,
    # ISBN 9780444511416,
    # https://doi.org/10.1016/S0169-7161(04)24010-X.
    
    # produce the MCD estimates
    energy.mcd <- robustbase::covMcd(energy)
    
    # center vectors using the MCD estimator
    xMinusM <- sweep(energy, 2L, energy.mcd$center)
    xMinusMTransposed <- t(xMinusM)
    
    if (inv.pseudo == TRUE) {
      inverseCovMatrix <- pinv(energy.mcd$cov)
    } else {
      # using normal inverse
      inverseCovMatrix <- solve(energy.mcd$cov)
    }
    
    left <- as.matrix(xMinusM) %*% as.matrix(inverseCovMatrix)
    mdSquared <- left %*% as.matrix(xMinusMTransposed)
    mdSquaredDiag <- diag(mdSquared)
    
  }
  else {
    # using classic mahalanobis
    xMinusM <- scale(energy, scale = FALSE)
    
    xMinusMTransposed <- t(xMinusM)
    
    if (inv.pseudo == TRUE) {
      inverseCovMatrix <- pinv(var(energy))
    } else {
      inverseCovMatrix <- solve(var(energy))
    }
    left <- xMinusM %*% inverseCovMatrix
    mdSquared <- left %*% xMinusMTransposed
    mdSquaredDiag <- diag(mdSquared)
  }
  
  distances <- sqrt(mdSquaredDiag)
  
  # defining threshold
  threshold <- sqrt(qchisq(tol, ncol(energy)))
  energy$isOutlier <- (distances >= threshold)
  
  if (plot == TRUE){
    plot(densityplot(distances))
    plot(histogram(distances))
    # plot(energy, col = energy$isOutlier)
    # legend(7,4.3,unique(energy$isOutlier),col=1:length(energy$isOutlier),pch=1)
    }
  
  
  
  wrapper <- NULL
  wrapper$distances <- distances
  wrapper$isOutlier <- energy$isOutlier
  wrapper$threshold <- threshold
  return(wrapper)
}

mdRobustDistances <- mahalanobis2(energy.continous, robust = TRUE, inv.pseudo = TRUE, plot = TRUE)
```

```{r}
mdRobustDistances <- mahalanobis2(energy.continous, robust = TRUE, inv.pseudo = FALSE, plot = TRUE)
```

We can assume there are no outliers.

```{r}
length(mdRobustDistances[mdRobustDistances$isOutlier == TRUE])
```



## Histograms
```{r}
par(mfrow=c(2, 5))
colnames <- colnames(energy)
for (i in 1:10) {
    hist(as.numeric(unlist(energy[,i])), main=colnames[i], probability=TRUE, col="gray", border="white")
}

```



## Check Correlations

```{r}
# not considering categorical variables
chart.Correlation(energy.continous, histogram=TRUE)
```

## Dealing with multicollinearity

The outlier detection here needs revision. We observed the following

1. the dataset has a near singular covariance matrix: this means that there are linearly dependant rows or attributes in the dataset. See

  1. <https://stats.stackexchange.com/questions/464477/how-to-perform-multivariate-analysis-on-datasets-with-singular-covariance-matrix?noredirect=1#comment858869_464477> for a question about this specific topic.
  2. <https://stats.stackexchange.com/questions/70899/what-correlation-makes-a-matrix-singular-and-what-are-implications-of-singularit/70910#70910> about the implications of having a singular covariance matrix.
2. This means that we are in presence of multicollinearity, e.g. the `vif` is not available.

```{r eval=TRUE}
hldata <- select(energy, -(y2.CoolingLoad))
cldata <- select(energy, -(y1.HeatingLoad))

library(car)
hlmodel <- lm(y1.HeatingLoad ~ . , data=hldata)
clmodel <- lm(y2.CoolingLoad ~ . , data=cldata)
```


```{r eval=FALSE}
car::vif(hlmodel) # raises an exception, there are aliased coefficients in the model
```


```{r}
summary(hlmodel)
```


```{r}
alias(hlmodel)
```

We get that roof area (`X4.RoofArea`) is correlated with `X2.SurfaceArea` and `X3.WallArea`. The alternatives we have here are

- drop `X4.RoofArea`, probably dropping Surface Area or Wall Area also
- transform `X4.RoofArea` into something uncorrelated. Probably merging those features into one.

For now, let's just drop `X4.RoofArea`. 

```{r}
hlmodel.2 <- lm(y1.HeatingLoad ~ . -X4.RoofArea , data=hldata)
clmodel.2 <- lm(y2.CoolingLoad ~ . -X4.RoofArea, data=cldata)
```

```{r}
alias(hlmodel.2) # doesn't seem to have collinearity
```

```{r}
energy.2 <- select(energy.continous, -(X4.RoofArea))
det(cov(energy.2))
# [1] 146.1685
```

By dropping X4, we obtain a non singular covariance matrix, and

```{r}
Moutlier(scale(energy.2, center = TRUE, scale = colSums(energy.2)),
         quantile = 0.999,
         plot = TRUE)
```

Using our custom method

```{r}
set.seed(42)
mdClassicDistances2 <- mahalanobis2(energy.2, robust = FALSE, inv.pseudo = FALSE, plot = TRUE, tol = 0.999)
mdRobustDistances2 <- mahalanobis2(energy.2, robust = TRUE, inv.pseudo = FALSE, plot = TRUE, tol = 0.999)
```
```{r}
crd <- data.frame(rn = rownames(energy.2), dc = mdClassicDistances2$distances, dr = mdRobustDistances2$distances)
threshold <- mdClassicDistances2$threshold

p <- ggplot(mapping = aes(x = dc, y = dr), data = crd) +   
  geom_point() +
  geom_hline(yintercept = threshold, color = "red") +
  geom_vline(xintercept = threshold, color = "red") +
  geom_text(aes(label = rownames(crd)), alpha=0.2, hjust = 0.4, vjust = -0.5) +
  xlab("Classic Mahalanobis Distance") +
  ylab("Robust Mahalanobis Distance") +
  labs(title = "Distance-distance plot") +
  labs(subtitle = "Outlier detection after removing X4.RoofArea")

p
```

# PCA

```{r}
library(FactoMineR)
library(factoextra)
library(plotrix)
library(gplots)
library(tableplot)

set.seed(42)
```


```{r}
targets <- grep("Load$", colnames(energy))
categoricals <- grep("^X[6,8]", colnames(energy))
PCA(energy, quali.sup = categoricals, quanti.sup = targets, scale.unit = TRUE)
```


